{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0919633a",
   "metadata": {},
   "source": [
    "# ğŸ§ª AI Model Playground\n",
    "\n",
    "**Test any HuggingFace model in minutes.** Just change `MODEL_ID` and run the cells.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d98aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once â€“ installs everything you need\n",
    "%pip install torch torchvision transformers huggingface-hub accelerate timm peft pillow sentencepiece safetensors requests rich click matplotlib pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb3257",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97196678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForImageClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    AutoImageProcessor,\n",
    "    AutoFeatureExtractor,\n",
    "    pipeline,\n",
    ")\n",
    "from huggingface_hub import HfApi, hf_hub_download, snapshot_download\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device:  {'MPS' if torch.backends.mps.is_available() else 'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d8bab",
   "metadata": {},
   "source": [
    "## 3. Configure Environment & Authentication\n",
    "\n",
    "Set your HuggingFace token if the model is gated (e.g. Llama, Gemma).  \n",
    "For public models you can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Device setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# â”€â”€ HuggingFace auth (uncomment if you need gated models) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# from huggingface_hub import login\n",
    "# login()  # will prompt for your token\n",
    "\n",
    "# Or set it directly:\n",
    "# os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\"\n",
    "\n",
    "# â”€â”€ Local cache dir â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b3190",
   "metadata": {},
   "source": [
    "## 4. Model Loading Utility\n",
    "\n",
    "A universal loader that auto-detects model type and returns the right model + processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb87e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map HuggingFace pipeline_tag â†’ best Auto class\n",
    "TASK_TO_MODEL_CLASS = {\n",
    "    \"image-classification\":      AutoModelForImageClassification,\n",
    "    \"text-classification\":       AutoModelForSequenceClassification,\n",
    "    \"sentiment-analysis\":        AutoModelForSequenceClassification,\n",
    "    \"text-generation\":           AutoModelForCausalLM,\n",
    "    \"text2text-generation\":      AutoModelForSeq2SeqLM,\n",
    "    \"feature-extraction\":        AutoModel,\n",
    "}\n",
    "\n",
    "def get_model_info(model_id: str) -> dict:\n",
    "    \"\"\"Fetch metadata from HuggingFace Hub.\"\"\"\n",
    "    api = HfApi()\n",
    "    info = api.model_info(model_id)\n",
    "    files = [s.rfilename for s in (info.siblings or [])]\n",
    "    return {\n",
    "        \"model_id\":     model_id,\n",
    "        \"pipeline_tag\": info.pipeline_tag or \"\",\n",
    "        \"library\":      info.library_name or \"\",\n",
    "        \"tags\":         info.tags or [],\n",
    "        \"files\":        files,\n",
    "    }\n",
    "\n",
    "def download_repo(model_id: str) -> Path:\n",
    "    \"\"\"Download full repo into models/ folder.\"\"\"\n",
    "    safe = model_id.replace(\"/\", \"__\")\n",
    "    local_dir = MODELS_DIR / safe\n",
    "    if local_dir.exists() and any(local_dir.iterdir()):\n",
    "        print(f\"âœ“ Already downloaded: {local_dir}\")\n",
    "        return local_dir\n",
    "    print(f\"â†“ Downloading {model_id} â€¦\")\n",
    "    snapshot_download(repo_id=model_id, local_dir=str(local_dir),\n",
    "                      ignore_patterns=[\"*.md\", \".gitattributes\"])\n",
    "    print(f\"âœ“ Done â†’ {local_dir}\")\n",
    "    return local_dir\n",
    "\n",
    "def load_any_model(model_id: str, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Universal loader. Returns (model_or_module, processor_or_None, info).\n",
    "\n",
    "    - Standard HF models â†’ returns (model, processor, info)\n",
    "    - Custom models (has model.py) â†’ downloads repo, imports module, returns (module, None, info)\n",
    "    - Pipeline fallback â†’ returns (pipeline, None, info)\n",
    "    \"\"\"\n",
    "    info = get_model_info(model_id)\n",
    "    task = info[\"pipeline_tag\"]\n",
    "    files = info[\"files\"]\n",
    "    print(f\"Model:  {model_id}\")\n",
    "    print(f\"Task:   {task or 'unknown'}\")\n",
    "    print(f\"Library: {info['library'] or 'unknown'}\")\n",
    "    print(f\"Files:  {len(files)}\")\n",
    "\n",
    "    # â”€â”€ Custom code models (model.py in repo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    has_custom = any(f in files for f in [\"model.py\", \"inference.py\", \"predict.py\"])\n",
    "    if has_custom:\n",
    "        local_dir = download_repo(model_id)\n",
    "        custom_file = next(f for f in [\"model.py\", \"inference.py\", \"predict.py\"] if f in files)\n",
    "        # Add to path so imports work\n",
    "        if str(local_dir) not in sys.path:\n",
    "            sys.path.insert(0, str(local_dir))\n",
    "        spec = importlib.util.spec_from_file_location(\"custom_model\", local_dir / custom_file)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        exports = [n for n in dir(module) if not n.startswith(\"_\") and callable(getattr(module, n))]\n",
    "        print(f\"\\nâœ“ Custom module loaded from {custom_file}\")\n",
    "        print(f\"  Available: {', '.join(exports)}\")\n",
    "        return module, None, info\n",
    "\n",
    "    # â”€â”€ Standard HF pipeline model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model_cls = TASK_TO_MODEL_CLASS.get(task)\n",
    "\n",
    "    if model_cls:\n",
    "        print(f\"â³ Loading with {model_cls.__name__} â€¦\")\n",
    "        model = model_cls.from_pretrained(model_id).to(device).eval()\n",
    "\n",
    "        # Try loading processor / tokenizer\n",
    "        processor = None\n",
    "        for loader in [AutoProcessor, AutoImageProcessor, AutoTokenizer, AutoFeatureExtractor]:\n",
    "            try:\n",
    "                processor = loader.from_pretrained(model_id)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"âœ“ Loaded â€” {n_params/1e6:.1f}M parameters on {device}\")\n",
    "\n",
    "        # Show labels if available\n",
    "        if hasattr(model.config, \"id2label\") and model.config.id2label:\n",
    "            labels = model.config.id2label\n",
    "            print(f\"  Labels ({len(labels)}): {dict(list(labels.items())[:5])} â€¦\")\n",
    "\n",
    "        return model, processor, info\n",
    "\n",
    "    # â”€â”€ Fallback: HF pipeline API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if task:\n",
    "        print(f\"â³ Loading via pipeline('{task}') â€¦\")\n",
    "        pipe = pipeline(task, model=model_id, device=device if str(device) != \"mps\" else -1)\n",
    "        print(f\"âœ“ Pipeline ready\")\n",
    "        return pipe, None, info\n",
    "\n",
    "    # â”€â”€ Last resort: download and let user figure it out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    local_dir = download_repo(model_id)\n",
    "    print(f\"âš  Unknown task type. Files downloaded to: {local_dir}\")\n",
    "    return None, None, info\n",
    "\n",
    "print(\"âœ“ Utility ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c844b",
   "metadata": {},
   "source": [
    "## 5. Load a Model\n",
    "\n",
    "**Change `MODEL_ID` to test any model.** Just paste a HuggingFace URL or `org/model` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CHANGE THIS to test any model                                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "MODEL_ID = \"Bombek1/ai-image-detector-siglip-dinov2\"\n",
    "\n",
    "# More examples â€“ uncomment any line:\n",
    "# MODEL_ID = \"google/vit-base-patch16-224\"              # image classification\n",
    "# MODEL_ID = \"microsoft/resnet-50\"                       # image classification\n",
    "# MODEL_ID = \"facebook/detr-resnet-50\"                   # object detection\n",
    "# MODEL_ID = \"distilbert-base-uncased-finetuned-sst-2-english\"  # sentiment\n",
    "# MODEL_ID = \"google/gemma-2-2b-it\"                     # text generation (needs HF token)\n",
    "# MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"         # text generation\n",
    "# MODEL_ID = \"openai/whisper-base\"                       # speech recognition\n",
    "\n",
    "model, processor, info = load_any_model(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93230e07",
   "metadata": {},
   "source": [
    "## 6. Prepare Sample Input\n",
    "\n",
    "Helper functions for loading images, text, or audio depending on the model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e69e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(source: str) -> Image.Image:\n",
    "    \"\"\"Load an image from a URL or local file path.\"\"\"\n",
    "    if source.startswith(\"http\"):\n",
    "        response = requests.get(source, timeout=15)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        img = Image.open(source).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def show_image(img: Image.Image, title: str = \"\"):\n",
    "    \"\"\"Display an image inline.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# â”€â”€ Load a sample image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SAMPLE_IMAGE_URL = \"https://images.unsplash.com/photo-1574158622682-e40e69881006?w=400\"  # cat photo\n",
    "\n",
    "sample_image = load_image(SAMPLE_IMAGE_URL)\n",
    "show_image(sample_image, \"Sample Input Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a09cb9",
   "metadata": {},
   "source": [
    "## 7. Run Inference\n",
    "\n",
    "This section handles both **standard HF models** and **custom models** (like the AI image detector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a62c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = info.get(\"pipeline_tag\", \"\")\n",
    "has_custom = any(f in info.get(\"files\", []) for f in [\"model.py\", \"inference.py\", \"predict.py\"])\n",
    "\n",
    "if has_custom:\n",
    "    # â”€â”€ Custom model (e.g. Bombek1/ai-image-detector-siglip-dinov2) â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"Custom model detected â€” using module from model.py\")\n",
    "    print(f\"Available: {[n for n in dir(model) if not n.startswith('_') and callable(getattr(model, n))]}\")\n",
    "\n",
    "    # For the AI image detector specifically:\n",
    "    if hasattr(model, \"AIImageDetector\"):\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id=MODEL_ID,\n",
    "            filename=\"pytorch_model.pt\"\n",
    "        )\n",
    "        detector = model.AIImageDetector(model_path)\n",
    "        result = detector.predict(SAMPLE_IMAGE_URL)\n",
    "        print(f\"\\nPrediction: {result['prediction']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "        print(f\"P(AI):      {result['probability']:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nâš  Check the available classes/functions above and call them manually.\")\n",
    "        print(\"  Example: result = model.SomeClass(args).predict(input)\")\n",
    "\n",
    "elif task in (\"image-classification\", \"object-detection\", \"image-segmentation\"):\n",
    "    # â”€â”€ Vision models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if processor is not None:\n",
    "        inputs = processor(images=sample_image, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "        top5 = probs.topk(5)\n",
    "        results = []\n",
    "        for score, idx in zip(top5.values, top5.indices):\n",
    "            label = model.config.id2label.get(idx.item(), f\"class_{idx.item()}\")\n",
    "            results.append({\"Label\": label, \"Confidence\": f\"{score.item():.4f}\"})\n",
    "        df = pd.DataFrame(results)\n",
    "        print(df.to_string(index=False))\n",
    "    else:\n",
    "        # Pipeline fallback\n",
    "        pipe = pipeline(task, model=MODEL_ID)\n",
    "        results = pipe(sample_image)\n",
    "        for r in results[:5]:\n",
    "            print(f\"  {r['label']}: {r['score']:.4f}\")\n",
    "\n",
    "elif task in (\"text-generation\", \"text2text-generation\"):\n",
    "    # â”€â”€ Text generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    prompt = \"Explain quantum computing in one paragraph.\"\n",
    "    if processor:\n",
    "        inputs = processor(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
    "        print(processor.decode(out[0], skip_special_tokens=True))\n",
    "    else:\n",
    "        result = model(prompt, max_new_tokens=200)\n",
    "        print(result[0][\"generated_text\"])\n",
    "\n",
    "elif task in (\"text-classification\", \"sentiment-analysis\"):\n",
    "    # â”€â”€ Text classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    text = \"I absolutely love this product, it changed my life!\"\n",
    "    if processor:\n",
    "        inputs = processor(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "        for idx, score in enumerate(probs):\n",
    "            label = model.config.id2label.get(idx, f\"class_{idx}\")\n",
    "            print(f\"  {label}: {score.item():.4f}\")\n",
    "    else:\n",
    "        result = model(text)\n",
    "        for r in result:\n",
    "            print(f\"  {r['label']}: {r['score']:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Task '{task}' â€” use the model object directly or try:\")\n",
    "    print(f\"  pipe = pipeline('{task}', model='{MODEL_ID}')\")\n",
    "    print(f\"  result = pipe(your_input)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857868b1",
   "metadata": {},
   "source": [
    "## 8. Display & Visualise Results\n",
    "\n",
    "For vision models: show the image with prediction overlay.  \n",
    "For text models: results are already printed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we got tabular results from a vision model, show as a bar chart\n",
    "if task in (\"image-classification\",) and not has_custom and 'df' in dir():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axes[0].imshow(sample_image)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_title(\"Input Image\")\n",
    "\n",
    "    df_plot = df.copy()\n",
    "    df_plot[\"Confidence\"] = df_plot[\"Confidence\"].astype(float)\n",
    "    axes[1].barh(df_plot[\"Label\"], df_plot[\"Confidence\"], color=\"steelblue\")\n",
    "    axes[1].set_xlabel(\"Confidence\")\n",
    "    axes[1].set_title(\"Top-5 Predictions\")\n",
    "    axes[1].invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif has_custom and 'result' in dir() and isinstance(result, dict):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(sample_image)\n",
    "    ax.axis(\"off\")\n",
    "    pred = result.get(\"prediction\", \"?\")\n",
    "    conf = result.get(\"confidence\", 0)\n",
    "    color = \"red\" if \"ai\" in str(pred).lower() else \"green\"\n",
    "    ax.set_title(f\"{pred} ({conf:.1%})\", fontsize=16, color=color)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"â„¹ Results displayed in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf4b77",
   "metadata": {},
   "source": [
    "## 9. Test with Your Own Input\n",
    "\n",
    "Paste your own image URL, file path, or text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(url_or_path: str):\n",
    "    \"\"\"One-call image tester â€” works with the currently loaded model.\"\"\"\n",
    "    img = load_image(url_or_path)\n",
    "\n",
    "    if has_custom and 'detector' in dir():\n",
    "        # Custom model (e.g. AI image detector)\n",
    "        r = detector.predict(url_or_path)\n",
    "        show_image(img, f\"{r['prediction']} ({r['confidence']:.1%})\")\n",
    "        return r\n",
    "\n",
    "    elif processor is not None and task in (\"image-classification\",):\n",
    "        # Standard HF image classifier\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "        top = probs.topk(5)\n",
    "        results = []\n",
    "        for s, i in zip(top.values, top.indices):\n",
    "            label = model.config.id2label.get(i.item(), f\"class_{i.item()}\")\n",
    "            results.append({\"Label\": label, \"Confidence\": f\"{s.item():.4f}\"})\n",
    "        show_image(img, results[0][\"Label\"])\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    elif task:\n",
    "        pipe = pipeline(task, model=MODEL_ID)\n",
    "        r = pipe(img)\n",
    "        show_image(img, str(r[0] if isinstance(r, list) else r))\n",
    "        return r\n",
    "\n",
    "def test_text(text: str):\n",
    "    \"\"\"One-call text tester â€” works with the currently loaded model.\"\"\"\n",
    "    if processor and task in (\"text-generation\", \"text2text-generation\"):\n",
    "        inputs = processor(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=200, temperature=0.7, do_sample=True)\n",
    "        return processor.decode(out[0], skip_special_tokens=True)\n",
    "    elif processor and task in (\"text-classification\", \"sentiment-analysis\"):\n",
    "        inputs = processor(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "        results = {model.config.id2label.get(i, f\"class_{i}\"): f\"{s:.4f}\" for i, s in enumerate(probs)}\n",
    "        return results\n",
    "    elif task:\n",
    "        pipe = pipeline(task, model=MODEL_ID)\n",
    "        return pipe(text)\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PASTE YOUR OWN INPUT BELOW                                                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# test_image(\"https://example.com/your-image.jpg\")\n",
    "# test_image(\"/path/to/local/image.png\")\n",
    "# test_text(\"Your custom text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b4887",
   "metadata": {},
   "source": [
    "## 10. Swap to a Different Model\n",
    "\n",
    "To try a completely different model, just change `MODEL_ID` and re-run from **Section 5** onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b471dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Quick swap: uncomment one and re-run from Section 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# MODEL_ID = \"google/vit-base-patch16-224\"                        # Image classification\n",
    "# MODEL_ID = \"microsoft/resnet-50\"                                 # Image classification\n",
    "# MODEL_ID = \"facebook/detr-resnet-50\"                             # Object detection\n",
    "# MODEL_ID = \"distilbert-base-uncased-finetuned-sst-2-english\"    # Sentiment analysis\n",
    "# MODEL_ID = \"google/gemma-2-2b-it\"                               # Text generation (needs token)\n",
    "# MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"                   # Text generation\n",
    "# MODEL_ID = \"openai/whisper-base\"                                 # Speech-to-text\n",
    "# MODEL_ID = \"facebook/bart-large-cnn\"                             # Summarisation\n",
    "# MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"             # Embeddings\n",
    "# MODEL_ID = \"Salesforce/blip-image-captioning-base\"              # Image captioning\n",
    "# MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"           # Image generation\n",
    "\n",
    "# model, processor, info = load_any_model(MODEL_ID)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
